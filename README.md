# ğŸ¤– Local Linux Assistant with Ollama

Asistente virtual desarrollado en Python que corre 100% local utilizando **Ollama**.

## ğŸš€ Requisitos
1. Tener [Ollama](https://ollama.com/) instalado en Linux.
2. Descargar el modelo: `ollama pull llama3`.

## ğŸ› ï¸ InstalaciÃ³n y Uso
```bash
# Clonar el repositorio
git clone <tu-url-de-github>
cd mi-asistente-ollama

# Configurar el entorno
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Ejecutar
python3 main.py
```
